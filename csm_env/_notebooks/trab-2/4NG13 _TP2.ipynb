{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "double-syndicate",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <div>\n",
    "        <img src=\"./report/isel_logo.png\" width=\"400\" height=\"400\" align=\"left\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <h2>Área Departamental de Engenharia de Eletrónica e Telecomunicações e de Computadores</h2>\n",
    "        <p>Trabalho prático 2</p>\n",
    "        <p>Autor:\t44598\tAndré L. A. Q. de Oliveira</p>\n",
    "        <p>Unidade Curricular Compressão de Sinais Multimédia</p>\n",
    "        <p>Professor: André Lourenço</p>\n",
    "        <p>09 - Maio - 2021</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-london",
   "metadata": {},
   "source": [
    "### <a id=\"index\"></a>\n",
    "\n",
    "# Index\n",
    "- [Codificação de Huffman](#codificacao_huffman)\n",
    "- [I/O Utilities](#io_utilities)\n",
    "    - [pad_bits](#pad_bits)\n",
    "    - [to_binary_list](#to_binary_list)\n",
    "    - [InputBitReader](#input_bit_reader)\n",
    "- [Tabela de Huffman](#tabela_huffman)\n",
    "    - [get_symbol_frequency](#get_symbol_frequency)\n",
    "    - [huff_node](#huff_node)\n",
    "    - [create_huff_tree](#create_huff_tree)\n",
    "    - [huff_tree_encode](#huff_tree_encode)\n",
    "    - [huff_tree_decode](#huff_tree_decode)\n",
    "    - [huff_tree2huff_dictionary](#huff_tree2huff_dictionary)\n",
    "    - [gen_huff_table](#gen_huff_table)\n",
    "- [Codificador de Huffman](#codificador_huffman)\n",
    "    - [encode_huff](#encode_huff)\n",
    "- [Descodificador de Huffman](#descodificador_huffman)\n",
    "    - [decode_huff](#decode_huff)\n",
    "- [Escrever para ficheiro](#escrever_ficheiro)\n",
    "    - [write2file](#write2file)\n",
    "- [Ler ficheiro](#ler_ficheiro)\n",
    "    - [read2array](#read2array)\n",
    "- [Testes](#testes)\n",
    "    - [a) gen_huff_table](#a)\n",
    "    - [b) calcular a eficiência](#b)\n",
    "    - [c) codificação da mensagem](#c)\n",
    "    - [d) gravar um ficheiro com a mensagem codificada](#d)\n",
    "    - [e) ler do ficheiro o conjunto de bits](#e)\n",
    "    - [f) descodificação da mensagem](#f)\n",
    "    - [g) comparar a mensagem descodificada com a original](#g)\n",
    "- [Conclusões](#conclusoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-dryer",
   "metadata": {},
   "source": [
    "<a id=\"codificacao_huffman\"></a>\n",
    "\n",
    "# Codificação de Huffman\n",
    "\n",
    "A codificação de Huffman é um método de compressão, desenvolvido em 1952 por  David A. Huffman, que usa as probabilidades de ocorrência dos símbolos nde um conjunto de dados a ser comprimido para determinar códigos binários de tamanho variável para cada símbolo.\n",
    "\n",
    "Uma árvore binária completa, chamada de árvore de Huffman é construída recursivamente a partir da junção dos dois símbolos de menor probabilidade, que são então somados em símbolos auxiliares que são depois recolocados no conjunto de símbolos. O processo termina quando todos os símbolos forem unidos em símbolos auxiliares, formando uma árvore binária. A árvore é então percorrida, atribuindo-se valores binários de 1 ou 0 para cada aresta, e os códigos são gerados a partir desse percurso.\n",
    "\n",
    "O resultado do algoritmo de Huffman pode ser visto como uma tabela de códigos de tamanho variável para codificar um símbolo. Os símbolos mais comuns são geralmente representados usando-se menos dígitos que os símbolos que aparecem com menos frequência.\n",
    "\n",
    "\n",
    "Para a string **\"go go gophers\"**, seria gerada a seguinte árvore de Huffman e respetiva tabela:\n",
    "\n",
    "![huff-table-example](./report/huff-table-example.PNG)\n",
    "\n",
    "A string seria codificada como: 000 001 111 000 001 111 000 001 010 011 100 101 110. Neste caso seriam utilizados três bits por caractere (em vez de oito bits por caractere como acontece no ASCII), a string **\"go go gophers\"** após codificação usaria um total de 39 bits em vez de 104 bits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-scratch",
   "metadata": {},
   "source": [
    "# Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "sharp-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "cwd = os.getcwd() # current work diretory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "massive-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.frombuffer('otorrinolaringologista'.encode('utf-8'), dtype='uint8')\n",
    "test2 = np.frombuffer('go go gophers'.encode('utf-8'), dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-welding",
   "metadata": {},
   "source": [
    "<a id=\"io_utilities\"></a>\n",
    "\n",
    "# I/O Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-operation",
   "metadata": {},
   "source": [
    "<a id=\"pad_bits\"></a>\n",
    "\n",
    "## pad_bits\n",
    "\n",
    "Extende o número de zeros a uma sequência de bits, para permitir codificação de tamanho fixo. Os zeros são adicionados nas posições de bit mais significantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "simple-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bits(bits, n):\n",
    "    # prefix string of bits with enough zeros to reach n digits\n",
    "    if isinstance(bits, np.ndarray):\n",
    "        if(n - len(bits) > 0):\n",
    "            return np.pad(bits, (n - len(bits), 0))\n",
    "        else:\n",
    "            return bits\n",
    "    else:\n",
    "        return ([0] * (n - len(bits)) + bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-designation",
   "metadata": {},
   "source": [
    "<a id=\"to_binary_list\"></a>\n",
    "\n",
    "## to_binary_list\n",
    "Converte um número inteiro na menor sequência de bits que o representa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "unable-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary_list(n):\n",
    "    # convert integer into a list of bits\n",
    "    return [n] if (n <= 1) else to_binary_list(n >> 1) + [n & 1]\n",
    "\n",
    "    #return [int(i) for i in list('{0:8b}'.format(n))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "powered-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(to_binary_list(320))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-private",
   "metadata": {},
   "source": [
    "<a id=\"input_bit_reader\"></a>\n",
    "\n",
    "## InputBitReader\n",
    "\n",
    "Para realizar compressão e descompressão com eficácia, é necessesário manipular os fluxos de dados como um fluxo de bits individuais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "rational-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBitReader(object): \n",
    "    def __init__(self, bit_seq): \n",
    "        self.bit_seq = bit_seq\n",
    "        self.size = len(bit_seq)\n",
    "        self.bits_read = 0\n",
    "        self.buffer = []\n",
    "\n",
    "    def read_bit(self):\n",
    "        if self.bits_read < self.size:\n",
    "            return self.read_bits(1)[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def read_bits(self, n):\n",
    "        self.__flush()\n",
    "        if self.bits_read < self.size:\n",
    "            self.buffer = pad_bits(self.bit_seq[self.bits_read:(self.bits_read + n)], n)\n",
    "            self.bits_read += n\n",
    "        return self.buffer\n",
    "    \n",
    "    def read_byte(self):\n",
    "        if self.bits_read < self.size:\n",
    "            byte = ''.join(list(map(str, self.read_bits(8))))\n",
    "            return int(byte, 2)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __flush(self):\n",
    "        self.buffer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-commissioner",
   "metadata": {},
   "source": [
    " <a id=\"tabela_huffman\"></a>\n",
    "\n",
    "# Tabela de Huffman\n",
    "\n",
    "A codificação de Huffman é um método de compressão que usa as probabilidades de ocorrência dos símbolos no conjunto de dados a ser comprimido para determinar códigos de tamanho variável para cada símbolo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-reputation",
   "metadata": {},
   "source": [
    " <a id=\"get_symbol_frequency\"></a>\n",
    "\n",
    "## get_symbol_frequency\n",
    "\n",
    "\n",
    "Lê um ficheiro, símbolo a símbolo, e retorna um dicionário com par chave-valor : símbolo-frequência, onde cada símbolo terá como respondência a sua frequência no ficheiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "rolled-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dicionary {symbol : frequency}\n",
    "def get_symbol_frequency(file):\n",
    "    d = dict()\n",
    "    for i in file:\n",
    "        d[i] = d.setdefault(i, 0) + 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-legend",
   "metadata": {},
   "source": [
    " <a id=\"huff_nome\"></a>\n",
    "\n",
    "## huff_node\n",
    "\n",
    "Classe que representa um nó de Huffman. Cada nó contêm a seguinte informação:\n",
    "* o símbolo\n",
    "* a frequência do símbolo\n",
    "* uma ligação para a esquerda e para a direita para os seus nós filhos\n",
    "* o valor de huffman atribuído quando o nó toma uma direção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "manual-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huffman Node\n",
    "class huff_node:\n",
    "    def __init__(self, symbol, freq, left = None, right = None):\n",
    "        # symbol\n",
    "        self.symbol = symbol\n",
    "        # frequency of symbol\n",
    "        self.freq = freq\n",
    "        # node left of current node\n",
    "        self.left = left\n",
    "        # node right of current node\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-mongolia",
   "metadata": {},
   "source": [
    " <a id=\"create_huff_tree\"></a>\n",
    "\n",
    "## create_huff_tree\n",
    "\n",
    "A árvore de Huffman é construída recursivamente a partir da junção dos dois símbolos de menor probabilidade, que são então somados em símbolos auxiliares e estes símbolos auxiliares recolocados no conjunto de símbolos. O processo termina quando todos os símbolos forem unidos em símbolos auxiliares, formando uma árvore binária.\n",
    "\n",
    "1. Com o valor de cada chave única presente no dicionário, são criados nós e colocados numa lista;\n",
    "2. São retirados os dois símbolos com menor frequência da lista, atribuindo-lhes o valor de 0 ou 1, e mergem-se esses dois símbolos num só, somando as suas freqûencias; \n",
    "3. O novo nó é adicionado a lista;\n",
    "4. O prodecimento repete-se até enquanto o número de nós for superior a 1;\n",
    "5. A função retorna o nó raiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "cheap-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huff_tree(file):\n",
    "    dictionary = get_symbol_frequency(file)\n",
    "    \n",
    "    tree = []\n",
    "    for symb, freq in dictionary.items():\n",
    "        tree.append(huff_node(symb, freq))\n",
    "    \n",
    "    while len(tree) > 1:\n",
    "        tree = sorted(tree, key=lambda n: n.freq)\n",
    "        # pop the 2 smallest nodes\n",
    "        left  = tree.pop(0)\n",
    "        right = tree.pop(0)\n",
    "        # combine the 2 smallest nodes to create new node as their parent\n",
    "        new_node = huff_node(str(left.symbol) + str(right.symbol), left.freq + right.freq, left, right)\n",
    "        tree.append(new_node)\n",
    "        \n",
    "    return tree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-owner",
   "metadata": {},
   "source": [
    "<a id=\"huff_tree2huff_dictionary\"></a>\n",
    "\n",
    "## huff_tree2huff_dictionary\n",
    "\n",
    "A partiz de uma árvore de Huffman gera um dicionário de Huffman. A árvore é percorrida, atribuindo-se valores binários de 1 ou 0 para cada nó, e os códigos são gerados a partir desse percurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "constant-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree2huff_dictionary(node):\n",
    "    d = dict()\n",
    "    huff_tree2huff_dictionary_aux(node, d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "determined-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree2huff_dictionary_aux(node, dictionary, value = \"\"):\n",
    "    if(node.left):\n",
    "        huff_tree2huff_dictionary_aux(node.left, dictionary, value + \"0\")\n",
    "    if(node.right):\n",
    "        huff_tree2huff_dictionary_aux(node.right, dictionary, value + \"1\")\n",
    "    # if node is leaf\n",
    "    if(not node.left and not node.right):\n",
    "        dictionary[node.symbol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "affecting-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': '00', 'o': '01', 's': '100', ' ': '101', 'p': '1100', 'h': '1101', 'e': '1110', 'r': '1111'}\n"
     ]
    }
   ],
   "source": [
    "huffman_tree = create_huff_tree('go go gophers')\n",
    "d = huff_tree2huff_dictionary(huffman_tree)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-findings",
   "metadata": {},
   "source": [
    "<a id=\"huff_tree_encode\"></a>\n",
    "\n",
    "## huff_tree_encode\n",
    "\n",
    "Além de compactar um ficheiro, é necessário também armazenar um cabeçalho no arquivo compactado que será utilizado pelo programa de descompactação. Em suma, é necessário de alguam forma, armazenar a árvore utilizada para compactar o ficheiro original. Esta necessidade deve-se ao facto de o programa de descompressão precisa também dessa mesma árvore para decodificar os dados.\n",
    "\n",
    "Para armazenar a árvore de huffman no cabeçalho do ficheiro, recore-se a estratégia de pesquisa em árvore \"post-order transversel\", para assinalar cada nó visitado. Ao encontrar um nó folha, é escrito o valor 1, seguido pelo símbolo do nó folha. Ao encontrar um nó wue não seja folha, é escrito o valor um 0.\n",
    "\n",
    "Considerando a mesma string utilizanda anteriormente como exemplo, **\"go go gophers\"**, a informação do cabeçalho ficaria expressa sepla seguinte codificação: **\"1g1o01s1 01p1h01e1r0000\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "smoking-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree_encode(node):\n",
    "    bits = []\n",
    "    huff_tree_encode_postorder(node, bits)\n",
    "    \n",
    "    huff_table = np.array([], dtype='uint8')\n",
    "    for bit in bits:\n",
    "        huff_table = np.append(huff_table, bit)\n",
    "    huff_table = np.packbits(huff_table)\n",
    "    \n",
    "    return huff_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "identical-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree_encode_postorder(node, bits):\n",
    "    if (node.left):\n",
    "        huff_tree_encode_postorder(node.left, bits)\n",
    "    if (node.right):\n",
    "        huff_tree_encode_postorder(node.right, bits)\n",
    "    # if node is leaf\n",
    "    if (not node.left and not node.right):\n",
    "        bits.append(1)\n",
    "        bits.append(np.unpackbits(node.symbol))\n",
    "    else:\n",
    "        bits.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "involved-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g1o01s1 01p1h01e1r0000\n"
     ]
    }
   ],
   "source": [
    "# for example demonstration purposes\n",
    "def huff_tree_encode_postorder_ascii_example(node, symbols):\n",
    "    if (node.left):\n",
    "        huff_tree_encode_postorder_ascii_example(node.left, symbols)\n",
    "    if (node.right):\n",
    "        huff_tree_encode_postorder_ascii_example(node.right, symbols)\n",
    "    # if node is leaf\n",
    "    if (not node.left and not node.right):\n",
    "        symbols.append(1)\n",
    "        symbols.append(node.symbol)\n",
    "    else:\n",
    "        symbols.append(0)\n",
    "\n",
    "huffman_tree = create_huff_tree('go go gophers')\n",
    "huff_table = []\n",
    "huff_tree_encode_postorder_ascii_example(huffman_tree, huff_table)\n",
    "print(''.join(list(map(str,huff_table))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-integrity",
   "metadata": {},
   "source": [
    "<a id=\"huff_tree_decode\"></a>\n",
    "\n",
    "## huff_tree_decode\n",
    "\n",
    "A construção da árvore de Huffman a partir do cabeçalho, é realizada com recurso a um stack. A informação do cabeçalho deve ser lida bit a bit. Quando se lê um bit com o valor 1, significa que se está perante um nó do tipo folha, então é lido o próximo byte e coloca-se o símbolo no stack. Quando um bit com o valor 0 é lido, se a pilha contém apenas um elemento, então toda a árvore de Huffman está construída. Caso contrário, deve haver mais de um elemento na pilha, então são retirados os dois primeiros elementos da pilha. O primeiro elemento do stack é um novo nó direito, e o segundo elemento do stack é um novo nó esquerdo. Um o nó pai é criado com os filhos nó esquerdo e direito recém-criados, e é colocado depois no stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "alleged-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree_decode(huff_table):\n",
    "    ibr = InputBitReader(np.unpackbits(huff_table))\n",
    "    tree = []\n",
    "    bits_read = 0\n",
    "    \n",
    "    while True:\n",
    "        if (ibr.read_bit() == 1):\n",
    "            byte = ibr.read_byte()\n",
    "            tree.append(huff_node(byte, 0, None, None))\n",
    "        else:\n",
    "            # if tree contains only 1 element, then its complete\n",
    "            if len(tree) == 1:\n",
    "                break\n",
    "            right = tree.pop()\n",
    "            left  = tree.pop()\n",
    "            tree.append(huff_node(0, 0, left, right))\n",
    "            \n",
    "    return tree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-nation",
   "metadata": {},
   "source": [
    "<a id=\"gen_huff_table\"></a>\n",
    "\n",
    "## gen_huff_table\n",
    "\n",
    "Gere todas as chamadas das funções para gerar a tabela e o dicionário de Huffman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "exotic-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_huff_table(file):\n",
    "    huff_tree = create_huff_tree(file)\n",
    "    huff_table = huff_tree_encode(huff_tree)\n",
    "    huff_dictionary = huff_tree2huff_dictionary(huff_tree)\n",
    "    return huff_table, huff_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "european-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{103: '00', 111: '01', 115: '100', 32: '101', 112: '1100', 104: '1101', 101: '1110', 114: '1111'} \n",
      "\n",
      "{103: '00', 111: '01', 115: '100', 32: '101', 112: '1100', 104: '1101', 101: '1110', 114: '1111'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate huff table from \"go go gophers\" file\n",
    "encoded_table, huff_dictionary = gen_huff_table(test2)\n",
    "print(huff_dictionary, '\\n')\n",
    "\n",
    "# re-construct huffman tree from huff table\n",
    "decoded_huffman_tree = huff_tree_decode(encoded_table)\n",
    "# get huffman dictionary from huff table\n",
    "new_huff_dictionary = huff_tree2huff_dictionary(decoded_huffman_tree)\n",
    "print(new_huff_dictionary, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-absence",
   "metadata": {},
   "source": [
    " <a id=\"codificador_huffman\"></a>\n",
    "\n",
    "# Codificador de Huffman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-developer",
   "metadata": {},
   "source": [
    "<a id=\"encode_huff\"></a>\n",
    "\n",
    "## encode_huff\n",
    "\n",
    "A partir de um dicionário de huffman, codifica uma mensagem. A mensagem é lida símbolo a símbolo, e o valor do símbolo é substituído pela sequência de bits presente no dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "sufficient-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_huff(file, huff_dictionary):\n",
    "    biq_seq = \"\"\n",
    "    for byte in file:\n",
    "        biq_seq += huff_dictionary[byte]\n",
    "    return list(map(int, biq_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "cathedral-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# generate huff table from \"go go gophers\" file\n",
    "t, d = gen_huff_table(test2)\n",
    "# encode \"go go gophers\" file\n",
    "biq_seq = encode_huff(test2, d)\n",
    "print(biq_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-atlanta",
   "metadata": {},
   "source": [
    " <a id=\"descodificador_huffman\"></a>\n",
    "\n",
    "# Descodificador de Huffman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-banner",
   "metadata": {},
   "source": [
    "<a id=\"decode_huff\"></a>\n",
    "\n",
    "## decode_huff\n",
    "\n",
    "A partir de uma sequência de bits (mensagem codificada) e uma tabela de huffman, retorne uma mensagem descodificada. A sequência é lida bit a bit, e vão sendo colocados num buffer. Quando a sequência de bits presente no buffer corresponder a um símbolo no dicionário, este é adicionado ao output e o buffer é limpo. O processo repte-se até que todos os bits sejam lidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "continued-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_huff (bit_seq, huff_table):\n",
    "    huff_tree = huff_tree_decode(huff_table)\n",
    "    huff_dictionary = huff_tree2huff_dictionary(huff_tree)\n",
    "    inverted_huff_dictionary = dict(map(reversed, huff_dictionary.items()))\n",
    "    buffer = \"\"\n",
    "    output = []\n",
    "    for bit in bit_seq:\n",
    "        buffer += str(bit)\n",
    "        if buffer in inverted_huff_dictionary:\n",
    "            output.append(inverted_huff_dictionary[buffer])\n",
    "            buffer = \"\"\n",
    "    return bytearray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "outside-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'go go gophers')\n",
      "[0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1]\n",
      "go go gophers\n"
     ]
    }
   ],
   "source": [
    "decoded_msg = decode_huff(biq_seq, encoded_table)\n",
    "print(decoded_msg)\n",
    "print(np.unpackbits(decoded_msg))\n",
    "print((str(decoded_msg, 'utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-duncan",
   "metadata": {},
   "source": [
    " <a id=\"escrever_ficheiro\"></a>\n",
    "\n",
    "# Escrever para ficheiro\n",
    "\n",
    "A escrita para um ficheiro é conseguida de maneira trivial, gravando primeiro um cabeçalho (tabela seguinte) com informação sobre o a codificação e seguidamente com a mensagem codificada.\n",
    "\n",
    "Byte [index]                     | Info\n",
    ":------------------------------  | :-----------------------------\n",
    "0                                | tamanho da tabela de huffman\n",
    "1 - huffman_table_size           | huffman \n",
    "huffman_table_size + 1           | tamanho da mensagem codificada\n",
    "huffman_table_size + 2 - eof     | mensagem codificada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-temple",
   "metadata": {},
   "source": [
    "<a id=\"write2file\"></a>\n",
    "\n",
    "## write2file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "equivalent-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2file(huff_table, bit_seq, filename):\n",
    "    output = np.array([], dtype='uint8')\n",
    "    \n",
    "    # huffman table size\n",
    "    t_size = len(huff_table)\n",
    "    while True:\n",
    "        if(t_size > 255):\n",
    "            output = np.append(output, np.packbits(pad_bits(to_binary_list(255), 8)))\n",
    "            t_size -= 255\n",
    "        else:\n",
    "            output = np.append(output, np.packbits(pad_bits(to_binary_list(t_size), 8)))\n",
    "            break\n",
    "    \n",
    "    # huffman table\n",
    "    output = np.append(output, huff_table)\n",
    "    \n",
    "    byte_seq = np.packbits(bit_seq)\n",
    "    # data_size\n",
    "    d_size = (len(byte_seq)*8) - len(bit_seq)\n",
    "    output = np.append(output, np.packbits(pad_bits(to_binary_list(d_size), 8)))\n",
    "            \n",
    "    # data\n",
    "    output = np.append(output, byte_seq)\n",
    "        \n",
    "    np.save(f\"{cwd}/output_data/{filename}\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-community",
   "metadata": {},
   "source": [
    " <a id=\"ler_ficheiro\"></a>\n",
    "\n",
    "# Ler ficheiro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-intake",
   "metadata": {},
   "source": [
    "<a id=\"read2array\"></a>\n",
    "\n",
    "## read2array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "ignored-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read2array(filename):\n",
    "    \n",
    "    # huffman table size\n",
    "    table_index = 0\n",
    "    table_size = 0\n",
    "    while True:\n",
    "        if(filename[table_index] == 255):\n",
    "            table_size += 255\n",
    "            table_index += 1\n",
    "        else:\n",
    "            table_size += filename[table_index]\n",
    "            break\n",
    "        \n",
    "    # huffman table\n",
    "    huff_table = filename[(table_index + 1):(table_size + 1)]\n",
    "    \n",
    "    # data size\n",
    "    data_size = filename[(table_size + 1)]\n",
    "    \n",
    "    # data\n",
    "    byte_seq = filename[(table_size + 1 + 1):]\n",
    "    # get bit array sequence from byte array\n",
    "    bit_seq = np.unpackbits(byte_seq)\n",
    "   # from the bit sequence we are not interested on the first data_size bits\n",
    "    # so we ignore the rest\n",
    "    bit_seq = bit_seq[:(len(bit_seq)-data_size)]\n",
    "    # return bit sequence with respective huff table\n",
    "    return bit_seq, huff_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "physical-procedure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = np.fromfile(f\"{cwd}/intput_data/DecUniversalDH.pdf\", dtype='uint8')\n",
    "\n",
    "huff_table, huff_dictionary = gen_huff_table(test2)\n",
    "bit_seq = encode_huff(test2, huff_dictionary)\n",
    "write2file(huff_table, bit_seq, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "welsh-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack bits:\n",
      " [0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1]\n",
      "go go gophers\n"
     ]
    }
   ],
   "source": [
    "encoded_file = np.load(f\"{cwd}/output_data/test.npy\")\n",
    "encoded_bit_seq, encoded_huff_table = read2array(encoded_file)\n",
    "\n",
    "decoded_byte_seq = decode_huff(encoded_bit_seq, encoded_huff_table)\n",
    "print(\"unpack bits:\\n\", np.unpackbits(decoded_byte_seq))\n",
    "print((str(decoded_byte_seq, 'utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-lease",
   "metadata": {},
   "source": [
    " <a id=\"testes\"></a>\n",
    "\n",
    "# Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-electronics",
   "metadata": {},
   "source": [
    "## Importar dados para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "final-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input file name\n",
    "input_file_name = [\n",
    "    'DecUniversalDH.txt',\n",
    "    'DecUniversalDH.pdf',\n",
    "    'HenryMancini-PinkPanther30s.mp3',\n",
    "    'HenryMancini-PinkPantherC.mid',\n",
    "    'LenaColor.tif',\n",
    "    'LenaGray.tif',\n",
    "]\n",
    "\n",
    "# input file path\n",
    "input_file_path = []\n",
    "for name in input_files_name:\n",
    "    input_file_path.append(f\"{cwd}/intput_data/{name}\")\n",
    "\n",
    "# input file size, bytes\n",
    "input_file_size = []\n",
    "for path in files_path:\n",
    "    input_file_size.append(os.stat(path).st_size)\n",
    "    \n",
    "# input file, uint8    \n",
    "input_file = []\n",
    "for path in input_file_path:\n",
    "    input_file.append(np.fromfile(path, dtype='uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-mustang",
   "metadata": {},
   "source": [
    "<a id=\"a\"></a>\n",
    "\n",
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "strong-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecUniversalDH.txt:\n",
      " file size [bytes]: 11930\n",
      " time to create huff table & dictionary [seconds]: 0.011967897415161133\n",
      "\n",
      "DecUniversalDH.pdf:\n",
      " file size [bytes]: 17524\n",
      " time to create huff table & dictionary [seconds]: 0.03786611557006836\n",
      "\n",
      "HenryMancini-PinkPanther30s.mp3:\n",
      " file size [bytes]: 236925\n",
      " time to create huff table & dictionary [seconds]: 0.11747932434082031\n",
      "\n",
      "HenryMancini-PinkPantherC.mid:\n",
      " file size [bytes]: 48049\n",
      " time to create huff table & dictionary [seconds]: 0.04689431190490723\n",
      "\n",
      "LenaColor.tif:\n",
      " file size [bytes]: 786572\n",
      " time to create huff table & dictionary [seconds]: 0.4120798110961914\n",
      "\n",
      "LenaGray.tif:\n",
      " file size [bytes]: 210122\n",
      " time to create huff table & dictionary [seconds]: 0.1250004768371582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "huff_table = []\n",
    "huff_dictionary = []\n",
    "\n",
    "for i in range(len(input_file)):\n",
    "    t_start = time()\n",
    "    ht, hd = gen_huff_table(input_file[i])\n",
    "    t_end = time()\n",
    "    \n",
    "    huff_table.append(ht)\n",
    "    huff_dictionary.append(hd)\n",
    "    \n",
    "    print(f\"{input_file_name[i]}:\")\n",
    "    print(f\" file size [bytes]: {input_file_size[i]}\")\n",
    "    print(f\" time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-satin",
   "metadata": {},
   "source": [
    "<a id=\"b\"></a>\n",
    "\n",
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-confusion",
   "metadata": {},
   "source": [
    "<a id=\"c\"></a>\n",
    "\n",
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "cleared-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecUniversalDH.txt:\n",
      " time to encode message [seconds]: 0.02693343162536621\n",
      "\n",
      "DecUniversalDH.pdf:\n",
      " time to encode message [seconds]: 0.04587745666503906\n",
      "\n",
      "HenryMancini-PinkPanther30s.mp3:\n",
      " time to encode message [seconds]: 0.8854165077209473\n",
      "\n",
      "HenryMancini-PinkPantherC.mid:\n",
      " time to encode message [seconds]: 0.11978340148925781\n",
      "\n",
      "LenaColor.tif:\n",
      " time to encode message [seconds]: 4.777643918991089\n",
      "\n",
      "LenaGray.tif:\n",
      " time to encode message [seconds]: 0.7045125961303711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bit_seq = []\n",
    "\n",
    "for i in range(len(input_file)):\n",
    "    t_start = time()\n",
    "    bs = encode_huff(input_file[i], huff_dictionary[i])\n",
    "    t_end = time()\n",
    "    \n",
    "    bit_seq.append(bs)\n",
    "    \n",
    "    print(f\"{input_file_name[i]}:\")\n",
    "    print(f\" time to encode message [seconds]: {t_end - t_start}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-holmes",
   "metadata": {},
   "source": [
    "<a id=\"d\"></a>\n",
    "\n",
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "silent-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file name\n",
    "output_file_name = []\n",
    "for i in range(len(input_file)):\n",
    "    write2file(huff_table[i], bit_seq[i], input_file_name[i])\n",
    "    output_file_name.append(f\"{input_file_name[i]}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "sunset-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output file path\n",
    "output_file_path = []\n",
    "for name in output_file_name:\n",
    "    output_file_path.append(f\"{cwd}/output_data/{name}\")\n",
    "\n",
    "# output file size, bytes\n",
    "output_file_size = []\n",
    "for path in output_file_path:\n",
    "    output_file_size.append(os.stat(path).st_size)\n",
    "    \n",
    "# output file, uint8    \n",
    "output_file = []\n",
    "for path in output_file_path:\n",
    "    output_file.append(np.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "functional-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecUniversalDH.txt.npy:\n",
      " original file size [bytes]: 11930\n",
      " compressed file size [bytes]: 6851\n",
      " size diference [bytes]: 5079\n",
      "\n",
      "DecUniversalDH.pdf.npy:\n",
      " original file size [bytes]: 17524\n",
      " compressed file size [bytes]: 15932\n",
      " size diference [bytes]: 1592\n",
      "\n",
      "HenryMancini-PinkPanther30s.mp3.npy:\n",
      " original file size [bytes]: 236925\n",
      " compressed file size [bytes]: 237183\n",
      " size diference [bytes]: -258\n",
      "\n",
      "HenryMancini-PinkPantherC.mid.npy:\n",
      " original file size [bytes]: 48049\n",
      " compressed file size [bytes]: 36523\n",
      " size diference [bytes]: 11526\n",
      "\n",
      "LenaColor.tif.npy:\n",
      " original file size [bytes]: 786572\n",
      " compressed file size [bytes]: 765569\n",
      " size diference [bytes]: 21003\n",
      "\n",
      "LenaGray.tif.npy:\n",
      " original file size [bytes]: 210122\n",
      " compressed file size [bytes]: 205090\n",
      " size diference [bytes]: 5032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output_file)):\n",
    "    print(f\"{output_file_name[i]}:\")\n",
    "    print(f\" original file size [bytes]: {input_file_size[i]}\")\n",
    "    print(f\" compressed file size [bytes]: {output_file_size[i]}\")\n",
    "    print(f\" size diference [bytes]: {input_file_size[i] - output_file_size[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-credits",
   "metadata": {},
   "source": [
    "<a id=\"e\"></a>\n",
    "\n",
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "three-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_bit_seq = []\n",
    "encoded_huff_table = []\n",
    "\n",
    "for i in range(len(output_file)):\n",
    "    ebs, eht = read2array(output_file[i])\n",
    "    encoded_bit_seq.append(ebs)\n",
    "    encoded_huff_table.append(eht)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-edwards",
   "metadata": {},
   "source": [
    "<a id=\"f\"></a>\n",
    "\n",
    "## f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "higher-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecUniversalDH.txt.npy:\n",
      " time to decode message [seconds]: 0.03490424156188965\n",
      "\n",
      "DecUniversalDH.pdf.npy:\n",
      " time to decode message [seconds]: 0.08079147338867188\n",
      "\n",
      "HenryMancini-PinkPanther30s.mp3.npy:\n",
      " time to decode message [seconds]: 1.2130987644195557\n",
      "\n",
      "HenryMancini-PinkPantherC.mid.npy:\n",
      " time to decode message [seconds]: 0.31694984436035156\n",
      "\n",
      "LenaColor.tif.npy:\n",
      " time to decode message [seconds]: 8.491950035095215\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_message = []\n",
    "\n",
    "for i in range(len(encoded_bit_seq)):\n",
    "    t_start = time()\n",
    "    dm = decode_huff(encoded_bit_seq[i], encoded_huff_table[i])\n",
    "    t_end = time()\n",
    "\n",
    "    decoded_message.append(dm)\n",
    "    \n",
    "    print(f\"{output_file_name[i]}:\")\n",
    "    print(f\" time to decode message [seconds]: {t_end - t_start}\")\n",
    "    print()\n",
    "    \n",
    "print(decoded_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-degree",
   "metadata": {},
   "source": [
    "<a id=\"g\"></a>\n",
    "\n",
    "## g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-colonial",
   "metadata": {},
   "source": [
    "<a id=\"conclusoes\"></a>\n",
    "\n",
    "# Conclusões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-islam",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csm_env",
   "language": "python",
   "name": "csm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
