{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "double-syndicate",
   "metadata": {},
   "source": [
    "<div>\n",
    "     <div>\n",
    "        <img src=\"./report/isel_logo.png\" width=\"400\" height=\"400\" align=\"left\">\n",
    "    </div>\n",
    "    <div>\n",
    "        <h2>Área Departamental de Engenharia de Eletrónica e Telecomunicações e de Computadores</h2>\n",
    "        <p>Trabalho prático 2</p>\n",
    "        <p>Autor:\t44598\tAndré L. A. Q. de Oliveira</p>\n",
    "        <p>Unidade Curricular Compressão de Sinais Multimédia</p>\n",
    "        <p>Professor: André Lourenço</p>\n",
    "        <p>9 – Maio – 2021</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-london",
   "metadata": {},
   "source": [
    "### <a id=\"index\"></a>\n",
    "\n",
    "# Index\n",
    "- [Codificação de Huffman](#codificacao_huffman)\n",
    "- [I/O Utilities](#io_utilities)\n",
    "    - [pad_bits](#pad_bits)\n",
    "    - [to_binary_list](#to_binary_list)\n",
    "    - [InputBitReader](#input_bit_reader)\n",
    "- [Tabela de Huffman](#tabela_huffman)\n",
    "    - [bytes_frequency](#bytes_frequency)\n",
    "    - [huff_node](#huff_node)\n",
    "    - [create_huff_tree](#create_huff_tree)\n",
    "    - [huff_tree_encode](#huff_tree_encode)\n",
    "    - [huff_tree_decode](#huff_tree_decode)\n",
    "    - [huff_tree2huff_dictionary](#huff_tree2huff_dictionary)\n",
    "    - [gen_huff_table](#gen_huff_table)\n",
    "- [Codificador de Huffman](#codificador_huffman)\n",
    "    - [encode_huff](#encode_huff)\n",
    "- [Descodificador de Huffman](#descodificador_huffman)\n",
    "    - [decode_huff](#decode_huff)\n",
    "- [Escreve para ficheiro](#escrever_ficheiro)\n",
    "    - [write2file](#write2file)\n",
    "- [Ler ficheiro](#ler_ficheiro)\n",
    "    - [read2array](#read2array)\n",
    "- [Testes](#testes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-dryer",
   "metadata": {},
   "source": [
    "<a id=\"codificacao_huffman\"></a>\n",
    "\n",
    "# Codificação de Huffman\n",
    "\n",
    "A codificação de Huffman é um método de compressão, desenvolvido em 1952 por  David A. Huffman, que usa as probabilidades de ocorrência dos símbolos nde um conjunto de dados a ser comprimido para determinar códigos binários de tamanho variável para cada símbolo.\n",
    "\n",
    "Uma árvore binária completa, chamada de árvore de Huffman é construída recursivamente a partir da junção dos dois símbolos de menor probabilidade, que são então somados em símbolos auxiliares que são depois recolocados no conjunto de símbolos. O processo termina quando todos os símbolos forem unidos em símbolos auxiliares, formando uma árvore binária. A árvore é então percorrida, atribuindo-se valores binários de 1 ou 0 para cada aresta, e os códigos são gerados a partir desse percurso.\n",
    "\n",
    "O resultado do algoritmo de Huffman pode ser visto como uma tabela de códigos de tamanho variável para codificar um símbolo. Os símbolos mais comuns são geralmente representados usando-se menos dígitos que os símbolos que aparecem com menos frequência.\n",
    "\n",
    "\n",
    "Para a string **\"go go gophers\"**, seria gerada a seguinte árvore de Huffman e respetiva tabela:\n",
    "\n",
    "![huff-table-example](./report/huff-table-example.PNG)\n",
    "\n",
    "A string seria codificada como: 000 001 111 000 001 111 000 001 010 011 100 101 110. Neste caso seriam utilizados três bits por caractere (em vez de oito bits por caractere como acontece no ASCII), a string **\"go go gophers\"** após codificação usaria um total de 39 bits em vez de 104 bits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-scratch",
   "metadata": {},
   "source": [
    "# Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharp-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "cwd = os.getcwd() # current work diretory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-electronics",
   "metadata": {},
   "source": [
    "# Importar dados para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "final-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.frombuffer('otorrinolaringologista'.encode('utf-8'), dtype='uint8')\n",
    "test2 = np.frombuffer('go go gophers'.encode('utf-8'), dtype='uint8')\n",
    "\n",
    "dec_universal_IDH_txt = np.fromfile(f\"{cwd}/data/DecUniversalDH.txt\", dtype='uint8')\n",
    "dec_universal_IDH_pdf = np.fromfile(f\"{cwd}/data/DecUniversalDH.pdf\", dtype='uint8')\n",
    "henry_mancini_mp3 = np.fromfile(f\"{cwd}/data/HenryMancini-PinkPanther30s.mp3\", dtype='uint8')\n",
    "henry_mancini_mid = np.fromfile(f\"{cwd}/data/HenryMancini-PinkPantherC.mid\", dtype='uint8')\n",
    "lena_color = np.fromfile(f\"{cwd}/data/LenaColor.tif\", dtype='uint8')\n",
    "lena_gray = np.fromfile(f\"{cwd}/data/LenaGray.tif\", dtype='uint8');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-welding",
   "metadata": {},
   "source": [
    "<a id=\"io_utilities\"></a>\n",
    "\n",
    "# I/O Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-operation",
   "metadata": {},
   "source": [
    "<a id=\"pad_bits\"></a>\n",
    "\n",
    "## pad_bits\n",
    "\n",
    "Extende o número de zeros a uma sequência de bits, para permitir codificação de tamanho fixo. Os zeros são adicionados nas posições de bit mais significantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "simple-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bits(bits, n):\n",
    "    # prefix string of bits with enough zeros to reach n digits\n",
    "    if isinstance(bits, np.ndarray):\n",
    "        if(n - len(bits) > 0):\n",
    "            return np.pad(bits, (n - len(bits), 0))\n",
    "        else:\n",
    "            return bits\n",
    "    else:\n",
    "        return ([0] * (n - len(bits)) + bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-designation",
   "metadata": {},
   "source": [
    "<a id=\"to_binary_list\"></a>\n",
    "\n",
    "## to_binary_list\n",
    "Converte um número inteiro na menor sequência de bits que o representa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unable-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary_list(n):\n",
    "    # Convert integer into a list of bits\n",
    "    return [n] if (n <= 1) else to_binary_list(n >> 1) + [n & 1]\n",
    "\n",
    "    # return [int(i) for i in list('{0:0b}'.format(n))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-private",
   "metadata": {},
   "source": [
    "<a id=\"input_bit_reader\"></a>\n",
    "\n",
    "## InputBitReader\n",
    "\n",
    "Para realizar compressão e descompressão com eficácia, é necessesário manipular os fluxos de dados como um fluxo de bits individuais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rational-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputBitReader(object): \n",
    "    def __init__(self, bit_seq): \n",
    "        self.bit_seq = bit_seq\n",
    "        self.size = len(bit_seq)\n",
    "        self.bits_read = 0\n",
    "        self.buffer = []\n",
    "\n",
    "    def read_bit(self):\n",
    "        if self.bits_read < self.size:\n",
    "            return self.read_bits(1)[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def read_bits(self, n):\n",
    "        self.__flush()\n",
    "        if self.bits_read < self.size:\n",
    "            self.buffer = pad_bits(self.bit_seq[self.bits_read:(self.bits_read + n)], n)\n",
    "            self.bits_read += n\n",
    "        return self.buffer\n",
    "    \n",
    "    def read_byte(self):\n",
    "        if self.bits_read < self.size:\n",
    "            byte = ''.join(list(map(str, self.read_bits(8))))\n",
    "            return int(byte, 2)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __flush(self):\n",
    "        self.buffer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-commissioner",
   "metadata": {},
   "source": [
    " <a id=\"tabela_huffman\"></a>\n",
    "\n",
    "# Tabela de Huffman\n",
    "\n",
    "A codificação de Huffman é um método de compressão que usa as probabilidades de ocorrência dos símbolos no conjunto de dados a ser comprimido para determinar códigos de tamanho variável para cada símbolo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-reputation",
   "metadata": {},
   "source": [
    " <a id=\"bytes_frequency\"></a>\n",
    "\n",
    "## bytes_frequency\n",
    "\n",
    "\n",
    "Lê um ficheiro, byte a byte, e retorna um dicionário com par chave-valor : símbolo-frequência, onde cada símbolo terá como respondência a sua frequência no ficheiro. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rolled-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return dicionary {byte : frequency}\n",
    "def bytes_frequency(file):\n",
    "    d = dict()\n",
    "    for byte in file:\n",
    "        d[byte] = d.setdefault(byte, 0) + 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-legend",
   "metadata": {},
   "source": [
    " <a id=\"huff_nome\"></a>\n",
    "\n",
    "## huff_node\n",
    "\n",
    "Classe que representa um nó de Huffman. Cada nó contêm a seguinte informação:\n",
    "* o símbolo\n",
    "* a frequência do símbolo\n",
    "* uma ligação para a esquerda e para a direita para os seus nós filhos\n",
    "* o valor de huffman atribuído quando o nó toma uma direção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "manual-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huffman Node\n",
    "class huff_node:\n",
    "    def __init__(self, symbol, freq, left = None, right = None):\n",
    "        # symbol\n",
    "        self.symbol = symbol\n",
    "        # frequency of symbol\n",
    "        self.freq = freq\n",
    "        # node left of current node\n",
    "        self.left = left\n",
    "        # node right of current node\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-mongolia",
   "metadata": {},
   "source": [
    " <a id=\"create_huff_tree\"></a>\n",
    "\n",
    "## create_huff_tree\n",
    "\n",
    "A árvore de Huffman é construída recursivamente a partir da junção dos dois símbolos de menor probabilidade, que são então somados em símbolos auxiliares e estes símbolos auxiliares recolocados no conjunto de símbolos. O processo termina quando todos os símbolos forem unidos em símbolos auxiliares, formando uma árvore binária.\n",
    "\n",
    "1. Com o valor de cada chave única presente no dicionário, são criados nós e colocados numa lista;\n",
    "2. São retirados os dois símbolos com menor frequência da lista, atribuindo-lhes o valor de 0 ou 1, e mergem-se esses dois símbolos num só, somando as suas freqûencias; \n",
    "3. O novo nó é adicionado a lista;\n",
    "4. O prodecimento repete-se até enquanto o número de nós for superior a 1;\n",
    "5. A função retorna o nó raiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cheap-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuffmanTree from dictionary of {symbol : frequency}\n",
    "def create_huff_tree(dictionary):\n",
    "    tree = []\n",
    "    for symb, freq in dictionary.items():\n",
    "        tree.append(huff_node(symb, freq))\n",
    "    \n",
    "    while len(tree) > 1:\n",
    "        tree = sorted(tree, key=lambda n: n.freq)\n",
    "        # pop the 2 smallest nodes\n",
    "        left = tree.pop(0)\n",
    "        right = tree.pop(0)\n",
    "        # combine the 2 smallest nodes to create new node as their parent\n",
    "        new_node = huff_node(str(left.symbol) + str(right.symbol), left.freq + right.freq, left, right)\n",
    "        tree.append(new_node)\n",
    "        \n",
    "    return tree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-owner",
   "metadata": {},
   "source": [
    "<a id=\"huff_tree2huff_dictionary\"></a>\n",
    "\n",
    "## huff_tree2huff_dictionary\n",
    "\n",
    "A partiz de uma árvore de Huffman gera uma tabela de Huffman. A árvore é percorrida, atribuindo-se valores binários de 1 ou 0 para cada aresta, e os códigos são gerados a partir desse percurso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constant-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree2huff_dictionary(node):\n",
    "    d = dict()\n",
    "    huff_tree2huff_dictionary_aux(node, d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "determined-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree2huff_dictionary_aux(node, dictionary, value = \"\"):\n",
    "    if(node.left):\n",
    "        huff_tree2huff_dictionary_aux(node.left, dictionary, value + \"0\")\n",
    "    if(node.right):\n",
    "        huff_tree2huff_dictionary_aux(node.right, dictionary, value + \"1\")\n",
    "    # if node is leaf\n",
    "    if(not node.left and not node.right):\n",
    "        dictionary[node.symbol] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "affecting-greeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g': '00', 'o': '01', 's': '100', ' ': '101', 'p': '1100', 'h': '1101', 'e': '1110', 'r': '1111'}\n"
     ]
    }
   ],
   "source": [
    "d = bytes_frequency('go go gophers')\n",
    "huffman_tree = create_huff_tree(d)\n",
    "d = huff_tree2huff_dictionary(root1)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-findings",
   "metadata": {},
   "source": [
    "<a id=\"huff_tree_encode\"></a>\n",
    "\n",
    "## huff_tree_encode\n",
    "\n",
    "Além de compactar um ficheiro, é necessário também armazenar um cabeçalho no arquivo compactado que será utilizado pelo programa de descompactação. Em suma, é necessário de alguam forma, armazenar a árvore utilizada para compactar o ficheiro original. Esta necessidade deve-se ao facto de o programa de descompressão precisa também dessa mesma árvore para decodificar os dados.\n",
    "\n",
    "Para armazenar a árvore de huffman no cabeçalho do ficheiro, recore-se a estratégia de pesquisa em árvore \"post-order transversel\", para assinalar cada nó visitado. Ao encontrar um nó folha, é escrito o valor 1, seguido pelo símbolo do nó folha. Ao encontrar um nó wue não seja folha, é escrito o valor um 0.\n",
    "\n",
    "Considerando a mesma string utilizanda anteriormente como exemplo, **\"go go gophers\"**, a informação do cabeçalho ficaria expressa sepla seguinte codificação: **\"1g1o01s1 01e1h01p1r0000\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "smoking-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree_encode(node):\n",
    "    bits = []\n",
    "    huff_tree_encode_postorder(node, bits)\n",
    "    \n",
    "    huff_table = np.array([], dtype='uint8')\n",
    "    for bit in bits:\n",
    "        huff_table = np.append(huff_table, bit)\n",
    "    huff_table = np.packbits(huff_table)\n",
    "    \n",
    "    return huff_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "identical-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree_encode_postorder(node, bits):\n",
    "    if (node.left):\n",
    "        huff_tree_encode_postorder(node.left, bits)\n",
    "    if (node.right):\n",
    "        huff_tree_encode_postorder(node.right, bits)\n",
    "    # if node is leaf\n",
    "    if (not node.left and not node.right):\n",
    "        bits.append(1)\n",
    "        bits.append(np.unpackbits(node.symbol))\n",
    "    else:\n",
    "        bits.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "involved-annotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g1o01s1 01p1h01e1r0000\n"
     ]
    }
   ],
   "source": [
    "def huff_tree_encode_postorder_ascii_example(node, symbols):\n",
    "    if (node.left):\n",
    "        huff_tree_encode_postorder_ascii_example(node.left, symbols)\n",
    "    if (node.right):\n",
    "        huff_tree_encode_postorder_ascii_example(node.right, symbols)\n",
    "    # if node is leaf\n",
    "    if (not node.left and not node.right):\n",
    "        symbols.append(1)\n",
    "        symbols.append(node.symbol)\n",
    "    else:\n",
    "        symbols.append(0)\n",
    "\n",
    "d = bytes_frequency('go go gophers')\n",
    "huffman_tree = create_huff_tree(d)\n",
    "huff_table = []\n",
    "huff_tree_encode_postorder_ascii_example(huffman_tree, huff_table)\n",
    "print(''.join(list(map(str,huff_table))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-integrity",
   "metadata": {},
   "source": [
    "<a id=\"huff_tree_decode\"></a>\n",
    "\n",
    "## huff_tree_decode\n",
    "\n",
    "A construção da árvore de Huffman a partir do cabeçalho, é realizada com recurso a um stack. A informação do cabeçalho deve ser lida bit a bit. Quando se lê um bit com o valor 1, significa que se está perante um nó do tipo folha, então é lido o próximo byte e coloca-se o símbolo no stack. Quando um bit com o valor 0 é lido, se a pilha contém apenas um elemento, então toda a árvore de Huffman está construída. Caso contrário, deve haver mais de um elemento na pilha, então são retirados os dois primeiros elementos da pilha. O primeiro elemento do stack é um novo nó direito, e o segundo elemento do stack é um novo nó esquerdo. Um o nó pai é criado com os filhos nó esquerdo e direito recém-criados, e é colocado depois no stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "alleged-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huff_tree_decode(huff_table):\n",
    "    ibr = InputBitReader(np.unpackbits(huff_table))\n",
    "    tree = []\n",
    "    bits_read = 0\n",
    "    \n",
    "    while True:\n",
    "        if (ibr.read_bit() == 1):\n",
    "            byte = ibr.read_byte()\n",
    "            tree.append(huff_node(byte, 0, None, None))\n",
    "        else:\n",
    "            # if tree contains only 1 element, then its complete\n",
    "            if len(tree) == 1:\n",
    "                break\n",
    "            right = tree.pop()\n",
    "            left = tree.pop()\n",
    "            tree.append(huff_node(0, 0, left, right))\n",
    "            \n",
    "    return tree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-nation",
   "metadata": {},
   "source": [
    "<a id=\"gen_huff_table\"></a>\n",
    "\n",
    "## gen_huff_table\n",
    "\n",
    "Função que agrega todas as chamadas para gerar a tabela de Huffman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exotic-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_huff_table(file):\n",
    "    d = bytes_frequency(file)\n",
    "    huff_tree = create_huff_tree(d)\n",
    "    huff_dictionary = huff_tree2huff_dictionary(huff_tree)\n",
    "    huff_table = huff_tree_encode(huff_tree)\n",
    "    return huff_table, huff_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "european-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{97: '000', 103: '001', 111: '01', 114: '100', 105: '101', 115: '1100', 116: '1101', 110: '1110', 108: '1111'} \n",
      "\n",
      "{97: '000', 103: '001', 111: '01', 114: '100', 105: '101', 115: '1100', 116: '1101', 110: '1110', 108: '1111'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate huff table\n",
    "encoded_table, huff_dictionary = gen_huff_table(test1)\n",
    "print(huff_dictionary, '\\n')\n",
    "\n",
    "# re-construct huffman tree from huff table\n",
    "decoded_huffman_tree = huff_tree_decode(encoded_table)\n",
    "# get huffman dictionary from huff table\n",
    "new_huff_dictionary = huff_tree2huff_dictionary(decoded_huffman_tree)\n",
    "print(new_huff_dictionary, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-absence",
   "metadata": {},
   "source": [
    " <a id=\"codificador_huffman\"></a>\n",
    "\n",
    "# Codificador de Huffman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-developer",
   "metadata": {},
   "source": [
    "<a id=\"encode_huff\"></a>\n",
    "\n",
    "## encode_huff\n",
    "\n",
    "Função que dada uma mensagem e uma tabela de Huffman, codifica uma mensagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sufficient-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_huff(file, huff_dictionary):\n",
    "    biq_seq = \"\"\n",
    "    for byte in file:\n",
    "        biq_seq += huff_dictionary[byte]\n",
    "    return list(map(int, biq_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cathedral-moscow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biq_seq \n",
      " [0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "biq_seq = encode_huff(test1, huff_dictionary)\n",
    "print(\"biq_seq \\n\", biq_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-atlanta",
   "metadata": {},
   "source": [
    " <a id=\"descodificador_huffman\"></a>\n",
    "\n",
    "# Descodificador de Huffman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-banner",
   "metadata": {},
   "source": [
    "<a id=\"decode_huff\"></a>\n",
    "\n",
    "## decode_huff\n",
    "\n",
    "Função que a partir de uma sequência de bits (mensagem codificada) e uma tabela de huffman, retorne uma mensagem descodificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "continued-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_huff (bit_seq, huff_table):\n",
    "    huff_tree = huff_tree_decode(huff_table)\n",
    "    huff_dictionary = huff_tree2huff_dictionary(huff_tree)\n",
    "    inverted_huff_dictionary = dict(map(reversed, huff_dictionary.items()))\n",
    "    buffer = \"\"\n",
    "    output = []\n",
    "    for bit in bit_seq:\n",
    "        buffer += str(bit)\n",
    "        if buffer in inverted_huff_dictionary:\n",
    "            output.append(inverted_huff_dictionary[buffer])\n",
    "            buffer = \"\"\n",
    "    return bytearray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "outside-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0\n",
      " 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1\n",
      " 1 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 0\n",
      " 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1]\n",
      "otorrinolaringologista\n"
     ]
    }
   ],
   "source": [
    "decoded_msg = decode_huff(biq_seq, huff_table)\n",
    "print(np.unpackbits(decoded_msg))\n",
    "print((str(decoded_msg, 'utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-duncan",
   "metadata": {},
   "source": [
    " <a id=\"escrever_ficheiro\"></a>\n",
    "\n",
    "# Escrever para ficheiro\n",
    "\n",
    "A escrita para um ficheiro, é conseguida de maneira trivial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-temple",
   "metadata": {},
   "source": [
    "<a id=\"write2file\"></a>\n",
    "\n",
    "## write2file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "confident-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2file(huff_table, bit_seq, filename): \n",
    "    \n",
    "    # calculate size of huffman table\n",
    "    t_size = pad_bits(to_binary_list(len(huff_table)), 8)\n",
    "    table_size = np.packbits(t_size)\n",
    "    \n",
    "    # calculate size of data\n",
    "    d_size = pad_bits(to_binary_list(len(bit_seq)), 8)\n",
    "    data_size = np.packbits(d_size)\n",
    "    \n",
    "    output = np.array([], dtype='uint8')\n",
    "    ### header info\n",
    "    # byte[0] : \n",
    "    #  size of huffman table\n",
    "    output = np.append(output, table_size)\n",
    "    # byte[1] - byte[table_size] : \n",
    "    #  huffman table\n",
    "    output = np.append(output, huff_table)\n",
    "    \n",
    "    ### data info\n",
    "    # byte[table_size + 1] : \n",
    "    #  size of data\n",
    "    output = np.append(output, data_size)\n",
    "    # byte[table_size + 1] - eof : \n",
    "    #   data\n",
    "    byte_seq = np.packbits(bit_seq)\n",
    "    output = np.append(output, byte_seq)\n",
    "    \n",
    "    np.save(f\"{cwd}/data/{filename}\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "physical-procedure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w-huff_table [179 219 215  57   2 225 104  89 110  64]\n",
      "w-data_size [37]\n",
      "w-bit_seq [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "w-byte_seq [ 26  52 115 123 224]\n"
     ]
    }
   ],
   "source": [
    "huff_table, huff_dictionary = gen_huff_table(test2)\n",
    "bit_seq = encode_huff(test2, huff_dictionary)\n",
    "\n",
    "write2file(huff_table, bit_seq, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-community",
   "metadata": {},
   "source": [
    " <a id=\"ler_ficheiro\"></a>\n",
    "\n",
    "# Ler ficheiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "executed-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read2array(filename):\n",
    "    # size of huffman table\n",
    "    table_size = filename[0]\n",
    "    # huffman table\n",
    "    huff_table = filename[1:(table_size + 1)]\n",
    "    # size of data\n",
    "    data_size = filename[(table_size + 1)]\n",
    "    # data\n",
    "    byte_seq = filename[(table_size + 1 + 1):]\n",
    "    # get bit array sequence from byte array\n",
    "    bit_seq = np.unpackbits(byte_seq)\n",
    "    # from the bit sequence we are only interested on the first data_size bits\n",
    "    # so we ignore the rest\n",
    "    bit_seq = bit_seq[:data_size]\n",
    "    # decode bit sequence using the huff table\n",
    "    return decode_huff(bit_seq, huff_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "affecting-terminal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table size 10\n",
      "huff table [179 219 215  57   2 225 104  89 110  64]\n",
      "data size 37\n",
      "byte seq [ 26  52 115 123 224]\n",
      "bit_seq [0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0\n",
      " 0 0 0]\n",
      "bit seq 37 [0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0]\n",
      "unpack [0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1\n",
      " 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 1 1]\n",
      "go go gophers\n"
     ]
    }
   ],
   "source": [
    "encoded_file = np.load(f\"{cwd}/data/test.npy\")\n",
    "byte_seq = read2array(encoded_file)\n",
    "print(\"unpack\", np.unpackbits(byte_seq))\n",
    "print((str(byte_seq, 'utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-lease",
   "metadata": {},
   "source": [
    " <a id=\"testes\"></a>\n",
    "\n",
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "strong-semiconductor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_universal_IDH_txt:\n",
      "file size [bytes]:  11930\n",
      "time to create huff table & dictionary: 0.00698089599609375\n",
      "\n",
      "dec_universal_IDH_pdf:\n",
      "file size [bytes]:  17524\n",
      "time to create huff table & dictionary: 0.024449586868286133\n",
      "\n",
      "henry_mancini_mp3:\n",
      "file size [bytes]:  236925\n",
      "time to create huff table & dictionary: 0.06876254081726074\n",
      "\n",
      "henry_mancini_mid:\n",
      "file size [bytes]:  48049\n",
      "time to create huff table & dictionary: 0.015991687774658203\n",
      "\n",
      "lena_color:\n",
      "file size [bytes]:  786572\n",
      "time to create huff table & dictionary: 0.20046234130859375\n",
      "\n",
      "lena_gray:\n",
      "file size [bytes]:  210122\n",
      "time to create huff table & dictionary: 0.052839040756225586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "huff_table, huff_dictionary = gen_huff_table(dec_universal_IDH_txt)\n",
    "t_end = time()\n",
    "print(\"dec_universal_IDH_txt:\")\n",
    "print(\"file size [bytes]: \", os.stat(f\"{cwd}/data/DecUniversalDH.txt\").st_size)\n",
    "print(f\"time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "print()\n",
    "\n",
    "t_start = time()\n",
    "huff_table, huff_dictionary = gen_huff_table(dec_universal_IDH_pdf)\n",
    "t_end = time()\n",
    "print(\"dec_universal_IDH_pdf:\")\n",
    "print(\"file size [bytes]: \", os.stat(f\"{cwd}/data/DecUniversalDH.pdf\").st_size)\n",
    "print(f\"time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "print()\n",
    "\n",
    "t_start = time()\n",
    "huff_table, huff_dictionary = gen_huff_table(henry_mancini_mp3)\n",
    "t_end = time()\n",
    "print(\"henry_mancini_mp3:\")\n",
    "print(\"file size [bytes]: \", os.stat(f\"{cwd}/data/HenryMancini-PinkPanther30s.mp3\").st_size)\n",
    "print(f\"time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "print()\n",
    "\n",
    "t_start = time()\n",
    "huff_table, huff_dictionary = gen_huff_table(henry_mancini_mid)\n",
    "t_end = time()\n",
    "print(\"henry_mancini_mid:\")\n",
    "print(\"file size [bytes]: \", os.stat(f\"{cwd}/data/HenryMancini-PinkPantherC.mid\").st_size)\n",
    "print(f\"time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "print()\n",
    "\n",
    "t_start = time()\n",
    "huff_table, huff_dictionary = gen_huff_table(lena_color)\n",
    "t_end = time()\n",
    "print(\"lena_color:\")\n",
    "print(\"file size [bytes]: \", os.stat(f\"{cwd}/data/LenaColor.tif\").st_size)\n",
    "print(f\"time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "print()\n",
    "\n",
    "t_start = time()\n",
    "huff_table, huff_dictionary = gen_huff_table(lena_gray)\n",
    "t_end = time()\n",
    "print(\"lena_gray:\")\n",
    "print(\"file size [bytes]: \", os.stat(f\"{cwd}/data/LenaGray.tif\").st_size)\n",
    "print(f\"time to create huff table & dictionary [seconds]: {t_end - t_start}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-episode",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csm_env",
   "language": "python",
   "name": "csm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
